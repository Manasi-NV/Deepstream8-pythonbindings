{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepStream 8.0 - Queue Sizing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GStreamer version: GStreamer 1.24.2\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append('/opt/nvidia/deepstream/deepstream-8.0/sources/deepstream_python_apps/apps')\n",
    "\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import GObject, Gst, GLib\n",
    "from common.bus_call import bus_call\n",
    "import pyds\n",
    "\n",
    "Gst.init(None)\n",
    "\n",
    "print(f\"GStreamer version: {Gst.version_string()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUEUE SIZING DETECTION WITH PEOPLENET\n",
      "============================================================\n",
      "Model: PeopleNet (person, bag, face)\n",
      "Input video: /app/notebooks/videos/queue_retail.mp4\n",
      "Output video: /app/notebooks/queue_sizing_out.mp4\n",
      "PGIE config: /app/notebooks/models/peoplenet/pgie_peoplenet_config.txt\n",
      "Tracker config: /app/notebooks/tracker_config.txt\n",
      "Analytics config: /app/notebooks/nvdsanalytics_config.txt\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Define class labels and file paths\n",
    "\n",
    "# Object class IDs (PeopleNet model)\n",
    "PGIE_CLASS_ID_PERSON = 0\n",
    "PGIE_CLASS_ID_BAG = 1\n",
    "PGIE_CLASS_ID_FACE = 2\n",
    "\n",
    "# ============================================================\n",
    "# INPUT/OUTPUT CONFIGURATION\n",
    "# ============================================================\n",
    "INPUT_VIDEO_FILE = '/app/notebooks/videos/queue_retail.mp4'  # Queue monitoring video\n",
    "OUTPUT_VIDEO_NAME = '/app/notebooks/queue_sizing_out.mp4'\n",
    "\n",
    "# Config files - Using PeopleNet for better person detection\n",
    "PGIE_CONFIG_FILE = '/app/notebooks/models/peoplenet/pgie_peoplenet_config.txt'\n",
    "TRACKER_CONFIG_FILE = '/app/notebooks/tracker_config.txt'\n",
    "ANALYTICS_CONFIG_FILE = '/app/notebooks/nvdsanalytics_config.txt'\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\"*60)\n",
    "print(\"QUEUE SIZING DETECTION WITH PEOPLENET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: PeopleNet (person, bag, face)\")\n",
    "print(f\"Input video: {INPUT_VIDEO_FILE}\")\n",
    "print(f\"Output video: {OUTPUT_VIDEO_NAME}\")\n",
    "print(f\"PGIE config: {PGIE_CONFIG_FILE}\")\n",
    "print(f\"Tracker config: {TRACKER_CONFIG_FILE}\")\n",
    "print(f\"Analytics config: {ANALYTICS_CONFIG_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_elm_or_print_err(factoryname, name, printedname, detail=\"\"):\n",
    "    \"\"\"Create a GStreamer element or print error message\"\"\"\n",
    "    print(f\"Creating {printedname}...\")\n",
    "    elm = Gst.ElementFactory.make(factoryname, name)\n",
    "    if not elm:\n",
    "        sys.stderr.write(f\"Unable to create {printedname}\\n\")\n",
    "    if detail:\n",
    "            sys.stderr.write(detail)\n",
    "    return elm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING PIPELINE\n",
      "============================================================\n",
      "Creating File Source...\n",
      "Creating QT Demuxer...\n",
      "Creating H264 Parser...\n",
      "Creating NV Decoder...\n",
      "Creating Stream Muxer...\n",
      "Creating Primary Inference (Person Detection)...\n",
      "Creating Object Tracker...\n",
      "Creating NV DS Analytics...\n",
      "Creating NV Video Converter 1...\n",
      "Creating On-Screen Display...\n",
      "Creating NV Video Converter 2...\n",
      "Creating Caps Filter...\n",
      "Creating H264 Encoder...\n",
      "Creating H264 Parser 2...\n",
      "Creating MP4 Muxer...\n",
      "Creating File Sink...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = Gst.Pipeline()\n",
    "if not pipeline:\n",
    "    sys.stderr.write(\"Unable to create Pipeline\\n\")\n",
    "\n",
    "# Create source elements for MP4 file\n",
    "source = make_elm_or_print_err(\"filesrc\", \"file-source\", \"File Source\")\n",
    "demux = make_elm_or_print_err(\"qtdemux\", \"qt-demux\", \"QT Demuxer\")\n",
    "\n",
    "# Create decode elements\n",
    "h264parser = make_elm_or_print_err(\"h264parse\", \"h264-parser\", \"H264 Parser\")\n",
    "decoder = make_elm_or_print_err(\"nvv4l2decoder\", \"nvv4l2-decoder\", \"NV Decoder\")\n",
    "streammux = make_elm_or_print_err(\"nvstreammux\", \"stream-muxer\", \"Stream Muxer\")\n",
    "\n",
    "# Create inference element\n",
    "pgie = make_elm_or_print_err(\"nvinfer\", \"primary-inference\", \"Primary Inference (Person Detection)\")\n",
    "\n",
    "# Create tracker element (for consistent object IDs across frames)\n",
    "tracker = make_elm_or_print_err(\"nvtracker\", \"tracker\", \"Object Tracker\")\n",
    "\n",
    "# Create analytics element (for ROI-based counting)\n",
    "analytics = make_elm_or_print_err(\"nvdsanalytics\", \"analytics\", \"NV DS Analytics\")\n",
    "\n",
    "# Create display and output elements\n",
    "nvvidconv = make_elm_or_print_err(\"nvvideoconvert\", \"convertor\", \"NV Video Converter 1\")\n",
    "nvosd = make_elm_or_print_err(\"nvdsosd\", \"onscreendisplay\", \"On-Screen Display\")\n",
    "nvvidconv2 = make_elm_or_print_err(\"nvvideoconvert\", \"convertor2\", \"NV Video Converter 2\")\n",
    "capsfilter = make_elm_or_print_err(\"capsfilter\", \"caps\", \"Caps Filter\")\n",
    "encoder = make_elm_or_print_err(\"nvv4l2h264enc\", \"encoder\", \"H264 Encoder\")\n",
    "h264parser2 = make_elm_or_print_err(\"h264parse\", \"h264-parser2\", \"H264 Parser 2\")\n",
    "mp4mux = make_elm_or_print_err(\"mp4mux\", \"mp4mux\", \"MP4 Muxer\")\n",
    "sink = make_elm_or_print_err(\"filesink\", \"filesink\", \"File Sink\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONFIGURING ELEMENTS\n",
      "============================================================\n",
      "Input file: /app/notebooks/videos/queue_retail.mp4\n",
      "Stream muxer: 640x360, batch-size=1\n",
      "PGIE config: /app/notebooks/models/peoplenet/pgie_peoplenet_config.txt\n",
      "Tracker: NvDCF tracker configured\n",
      "Analytics config: /app/notebooks/nvdsanalytics_config.txt\n",
      "Caps filter: I420 format\n",
      "Encoder bitrate: 4 Mbps\n",
      "Output file: /app/notebooks/queue_sizing_out.mp4\n",
      "\n",
      "All elements configured!\n"
     ]
    }
   ],
   "source": [
    "# Configure element properties\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIGURING ELEMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# File source configuration\n",
    "source.set_property('location', INPUT_VIDEO_FILE)\n",
    "print(f\"Input file: {INPUT_VIDEO_FILE}\")\n",
    "\n",
    "# Streammux: Set batch properties (640x360 for retail video)\n",
    "streammux.set_property('width', 640)\n",
    "streammux.set_property('height', 360)\n",
    "streammux.set_property('batch-size', 1)\n",
    "streammux.set_property('batched-push-timeout', 4000000)\n",
    "print(\"Stream muxer: 640x360, batch-size=1\")\n",
    "\n",
    "# Primary inference: Set config file\n",
    "pgie.set_property('config-file-path', PGIE_CONFIG_FILE)\n",
    "print(f\"PGIE config: {PGIE_CONFIG_FILE}\")\n",
    "\n",
    "# Tracker: Set config file\n",
    "tracker.set_property('ll-lib-file', '/opt/nvidia/deepstream/deepstream-8.0/lib/libnvds_nvmultiobjecttracker.so')\n",
    "tracker.set_property('ll-config-file', '/opt/nvidia/deepstream/deepstream-8.0/samples/configs/deepstream-app/config_tracker_NvDCF_perf.yml')\n",
    "tracker.set_property('tracker-width', 640)\n",
    "tracker.set_property('tracker-height', 480)\n",
    "tracker.set_property('display-tracking-id', 1)\n",
    "print(\"Tracker: NvDCF tracker configured\")\n",
    "\n",
    "# Analytics: Set config file for ROI-based counting\n",
    "analytics.set_property('config-file', ANALYTICS_CONFIG_FILE)\n",
    "print(f\"Analytics config: {ANALYTICS_CONFIG_FILE}\")\n",
    "\n",
    "# Capsfilter: Set format for encoder\n",
    "caps = Gst.Caps.from_string(\"video/x-raw(memory:NVMM), format=I420\")\n",
    "capsfilter.set_property(\"caps\", caps)\n",
    "print(\"Caps filter: I420 format\")\n",
    "\n",
    "# Encoder: Set bitrate\n",
    "encoder.set_property('bitrate', 4000000)\n",
    "print(\"Encoder bitrate: 4 Mbps\")\n",
    "\n",
    "# Sink: Set output file\n",
    "sink.set_property('location', OUTPUT_VIDEO_NAME)\n",
    "sink.set_property('sync', False)\n",
    "print(f\"Output file: {OUTPUT_VIDEO_NAME}\")\n",
    "\n",
    "print(\"\\nAll elements configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BUILDING PIPELINE\n",
      "============================================================\n",
      "Adding elements to pipeline...\n",
      "All elements added (including tracker and analytics)\n",
      "\n",
      "Linking elements...\n",
      "Linked: source â†’ demux\n",
      "Demux pad-added callback connected\n",
      "Linked: h264parser â†’ decoder\n",
      "Linked: decoder â†’ streammux\n",
      "Linked: streammux â†’ pgie â†’ tracker â†’ analytics â†’ nvvidconv â†’ nvosd â†’ encoder â†’ sink\n",
      "\n",
      "Pipeline built successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add elements to pipeline and link them\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILDING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Callback for qtdemux dynamic pad linking (MP4 files have dynamic pads)\n",
    "def on_demux_pad_added(demux, pad, h264parser):\n",
    "    \"\"\"Called when qtdemux creates a new pad (when video stream is found)\"\"\"\n",
    "    pad_name = pad.get_name()\n",
    "    print(f\"Demux pad added: {pad_name}\")\n",
    "    \n",
    "    # Only link video pads (ignore audio)\n",
    "    if pad_name.startswith(\"video\"):\n",
    "        sink_pad = h264parser.get_static_pad(\"sink\")\n",
    "        if not sink_pad.is_linked():\n",
    "            pad.link(sink_pad)\n",
    "            print(\"Demux linked to h264parser\")\n",
    "\n",
    "# Add all elements to pipeline\n",
    "print(\"Adding elements to pipeline...\")\n",
    "pipeline.add(source)\n",
    "pipeline.add(demux)\n",
    "pipeline.add(h264parser)\n",
    "pipeline.add(decoder)\n",
    "pipeline.add(streammux)\n",
    "pipeline.add(pgie)\n",
    "pipeline.add(tracker)      # NEW: Add tracker\n",
    "pipeline.add(analytics)    # NEW: Add analytics\n",
    "pipeline.add(nvvidconv)\n",
    "pipeline.add(nvosd)\n",
    "pipeline.add(nvvidconv2)\n",
    "pipeline.add(capsfilter)\n",
    "pipeline.add(encoder)\n",
    "pipeline.add(h264parser2)\n",
    "pipeline.add(mp4mux)\n",
    "pipeline.add(sink)\n",
    "print(\"All elements added (including tracker and analytics)\")\n",
    "\n",
    "# Link elements\n",
    "print(\"\\nLinking elements...\")\n",
    "\n",
    "# Link source â†’ demux (static)\n",
    "source.link(demux)\n",
    "print(\"Linked: source â†’ demux\")\n",
    "\n",
    "# Connect demux pad-added callback for dynamic linking\n",
    "demux.connect(\"pad-added\", on_demux_pad_added, h264parser)\n",
    "print(\"Demux pad-added callback connected\")\n",
    "\n",
    "# Link h264parser â†’ decoder (static)\n",
    "h264parser.link(decoder)\n",
    "print(\"Linked: h264parser â†’ decoder\")\n",
    "\n",
    "# Create pads for streammux\n",
    "sinkpad = streammux.request_pad_simple(\"sink_0\")\n",
    "if not sinkpad:\n",
    "    sys.stderr.write(\"Unable to get sink pad of streammux\\n\")\n",
    "srcpad = decoder.get_static_pad(\"src\")\n",
    "if not srcpad:\n",
    "    sys.stderr.write(\"Unable to get source pad of decoder\\n\")\n",
    "srcpad.link(sinkpad)\n",
    "print(\"Linked: decoder â†’ streammux\")\n",
    "\n",
    "# Link remaining elements (NEW: added tracker and analytics)\n",
    "streammux.link(pgie)\n",
    "pgie.link(tracker)\n",
    "tracker.link(analytics)\n",
    "analytics.link(nvvidconv)\n",
    "nvvidconv.link(nvosd)\n",
    "nvosd.link(nvvidconv2)\n",
    "nvvidconv2.link(capsfilter)\n",
    "capsfilter.link(encoder)\n",
    "encoder.link(h264parser2)\n",
    "h264parser2.link(mp4mux)\n",
    "mp4mux.link(sink)\n",
    "print(\"Linked: streammux â†’ pgie â†’ tracker â†’ analytics â†’ nvvidconv â†’ nvosd â†’ encoder â†’ sink\")\n",
    "\n",
    "print(\"\\nPipeline built successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queue counting: OVERLAP-BASED with filters\n",
      "ROI: (20,72) to (640,300)\n",
      "Filters: min_box=50x80, cashier_zone=X>490\n"
     ]
    }
   ],
   "source": [
    "# Define metadata probe function for queue counting with OVERLAP-BASED logic\n",
    "\n",
    "# ROI coordinates for queue area\n",
    "ROI_LEFT = 20\n",
    "ROI_TOP = 72\n",
    "ROI_RIGHT = 640\n",
    "ROI_BOTTOM = 300\n",
    "\n",
    "# Filters\n",
    "MIN_OVERLAP_RATIO = 0.3\n",
    "MIN_BOX_WIDTH = 50\n",
    "MIN_BOX_HEIGHT = 80\n",
    "MIN_BOX_BOTTOM_Y = 200\n",
    "CASHIER_ZONE_X = 490\n",
    "CASHIER_ZONE_BOTTOM = 350\n",
    "\n",
    "def calculate_overlap_ratio(box_left, box_top, box_width, box_height):\n",
    "    box_right = box_left + box_width\n",
    "    box_bottom = box_top + box_height\n",
    "    inter_left = max(box_left, ROI_LEFT)\n",
    "    inter_top = max(box_top, ROI_TOP)\n",
    "    inter_right = min(box_right, ROI_RIGHT)\n",
    "    inter_bottom = min(box_bottom, ROI_BOTTOM)\n",
    "    if inter_left >= inter_right or inter_top >= inter_bottom:\n",
    "        return 0.0\n",
    "    inter_area = (inter_right - inter_left) * (inter_bottom - inter_top)\n",
    "    box_area = box_width * box_height\n",
    "    if box_area <= 0:\n",
    "        return 0.0\n",
    "    return inter_area / box_area\n",
    "\n",
    "def is_foreground_person(box_left, box_top, box_width, box_height, box_bottom):\n",
    "    if box_width < MIN_BOX_WIDTH or box_height < MIN_BOX_HEIGHT:\n",
    "        return False\n",
    "    if box_bottom < MIN_BOX_BOTTOM_Y:\n",
    "        return False\n",
    "    if box_left > CASHIER_ZONE_X and box_bottom > CASHIER_ZONE_BOTTOM:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def osd_sink_pad_buffer_probe(pad, info, u_data):\n",
    "    \"\"\"Callback function to count people in queue using OVERLAP-BASED counting\"\"\"\n",
    "    \n",
    "    frame_number = 0\n",
    "    people_in_queue = 0\n",
    "    total_people = 0\n",
    "    \n",
    "    gst_buffer = info.get_buffer()\n",
    "    if not gst_buffer:\n",
    "        print(\"Unable to get GstBuffer\")\n",
    "        return Gst.PadProbeReturn.OK\n",
    "\n",
    "    # Retrieve batch metadata from buffer\n",
    "    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n",
    "    l_frame = batch_meta.frame_meta_list\n",
    "    \n",
    "    while l_frame is not None:\n",
    "        try:\n",
    "            frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        frame_number = frame_meta.frame_num\n",
    "        \n",
    "        # Count persons using OVERLAP-BASED logic with filters\n",
    "        l_obj = frame_meta.obj_meta_list\n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                obj_meta = pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "                if obj_meta.class_id == PGIE_CLASS_ID_PERSON:\n",
    "                    total_people += 1\n",
    "                    rect = obj_meta.rect_params\n",
    "                    box_bottom = rect.top + rect.height\n",
    "                    \n",
    "                    # Skip background/small people and cashier zone\n",
    "                    if not is_foreground_person(rect.left, rect.top, rect.width, rect.height, box_bottom):\n",
    "                        try:\n",
    "                            l_obj = l_obj.next\n",
    "                        except StopIteration:\n",
    "                            break\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate overlap with ROI\n",
    "                    overlap = calculate_overlap_ratio(rect.left, rect.top, rect.width, rect.height)\n",
    "                    if overlap >= MIN_OVERLAP_RATIO:\n",
    "                        people_in_queue += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "            try:\n",
    "                l_obj = l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        # Add display metadata\n",
    "        display_meta = pyds.nvds_acquire_display_meta_from_pool(batch_meta)\n",
    "        display_meta.num_labels = 1\n",
    "        py_nvosd_text_params = display_meta.text_params[0]\n",
    "        \n",
    "        # Set display text - show queue count prominently\n",
    "        py_nvosd_text_params.display_text = \"QUEUE COUNT: {} | Frame: {} | Total Detected: {}\".format(\n",
    "            people_in_queue, frame_number, total_people\n",
    "        )\n",
    "        \n",
    "        # Position and style\n",
    "        py_nvosd_text_params.x_offset = 10\n",
    "        py_nvosd_text_params.y_offset = 40\n",
    "        py_nvosd_text_params.font_params.font_name = \"Serif\"\n",
    "        py_nvosd_text_params.font_params.font_size = 12\n",
    "        py_nvosd_text_params.font_params.font_color.set(1.0, 1.0, 0.0, 1.0)  # Yellow\n",
    "        py_nvosd_text_params.set_bg_clr = 1\n",
    "        py_nvosd_text_params.text_bg_clr.set(0.0, 0.0, 0.0, 0.7)\n",
    "        \n",
    "        # Draw ROI rectangle (green box showing the queue area)\n",
    "        display_meta.num_rects = 0\n",
    "        rect_params = display_meta.rect_params[0]\n",
    "        rect_params.left = ROI_LEFT\n",
    "        rect_params.top = ROI_TOP\n",
    "        rect_params.width = ROI_RIGHT - ROI_LEFT\n",
    "        rect_params.height = ROI_BOTTOM - ROI_TOP\n",
    "        rect_params.border_width = 3\n",
    "        rect_params.border_color.set(0.0, 1.0, 0.0, 1.0)  # Green border\n",
    "        rect_params.has_bg_color = 0\n",
    "        \n",
    "        # Print to console every 50 frames\n",
    "        if frame_number % 50 == 0:\n",
    "            print(f\"Frame {frame_number}: Queue={people_in_queue}, Total={total_people}\")\n",
    "        \n",
    "        pyds.nvds_add_display_meta_to_frame(frame_meta, display_meta)\n",
    "        \n",
    "        try:\n",
    "            l_frame = l_frame.next\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    return Gst.PadProbeReturn.OK\n",
    "\n",
    "print(\"Queue counting: OVERLAP-BASED with filters\")\n",
    "print(f\"ROI: ({ROI_LEFT},{ROI_TOP}) to ({ROI_RIGHT},{ROI_BOTTOM})\")\n",
    "print(f\"Filters: min_box={MIN_BOX_WIDTH}x{MIN_BOX_HEIGHT}, cashier_zone=X>{CASHIER_ZONE_X}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata probe attached to OSD element\n"
     ]
    }
   ],
   "source": [
    "# Attach probe to OSD element\n",
    "\n",
    "osdsinkpad = nvosd.get_static_pad(\"sink\")\n",
    "if not osdsinkpad:\n",
    "    sys.stderr.write(\"Unable to get sink pad of nvosd\\n\")\n",
    "else:\n",
    "    osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0)\n",
    "    print(\"Metadata probe attached to OSD element\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bus message handler configured\n"
     ]
    }
   ],
   "source": [
    "# Setup bus message handler\n",
    "\n",
    "# Create event loop\n",
    "loop = GLib.MainLoop()\n",
    "bus = pipeline.get_bus()\n",
    "bus.add_signal_watch()\n",
    "bus.connect(\"message\", bus_call, loop)\n",
    "\n",
    "print(\"Bus message handler configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING QUEUE SIZING PIPELINE\n",
      "============================================================\n",
      "Input: /app/notebooks/videos/queue_retail.mp4\n",
      "Output: /app/notebooks/queue_sizing_out.mp4\n",
      "ROI: 60% of frame (center)\n",
      "Counting: People inside green ROI box\n",
      "============================================================\n",
      "\n",
      "Opening in BLOCKING MODE \n",
      "Opening in BLOCKING MODE \n",
      "gstnvtracker: Loading low-level lib at /opt/nvidia/deepstream/deepstream-8.0/lib/libnvds_nvmultiobjecttracker.so\n",
      "[NvMultiObjectTracker] Initialized\n",
      "Demux pad added: video_0\n",
      "Demux linked to h264parser\n",
      "Demux pad added: audio_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:00:00.622263633 \u001b[35m  622\u001b[00m     0x42f198a0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:685:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::deserializeEngineAndBackend() <nvdsinfer_context_impl.cpp:2109> [UID = 1]: deserialized trt engine from :/app/notebooks/models/peoplenet/resnet34_peoplenet_int8.onnx_b1_gpu0_fp16.engine\n",
      "0:00:00.622306108 \u001b[35m  622\u001b[00m     0x42f198a0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:685:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::generateBackendContext() <nvdsinfer_context_impl.cpp:2212> [UID = 1]: Use deserialized engine model: /app/notebooks/models/peoplenet/resnet34_peoplenet_int8.onnx_b1_gpu0_fp16.engine\n",
      "0:00:00.625375893 \u001b[35m  622\u001b[00m     0x42f198a0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer_impl.cpp:343:notifyLoadModelStatus:<primary-inference>\u001b[00m [UID 1]: Load new model:/app/notebooks/models/peoplenet/pgie_peoplenet_config.txt sucessfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: Queue=0, Total=0\n",
      "Frame 50: Queue=0, Total=0\n",
      "Frame 100: Queue=0, Total=0\n",
      "Frame 150: Queue=2, Total=6\n",
      "Frame 200: Queue=3, Total=7\n",
      "Frame 250: Queue=3, Total=7\n",
      "Frame 300: Queue=3, Total=7\n",
      "Frame 350: Queue=4, Total=8\n",
      "Frame 400: Queue=4, Total=8\n",
      "Frame 450: Queue=4, Total=10\n",
      "Frame 500: Queue=4, Total=10\n",
      "Frame 550: Queue=4, Total=10\n",
      "Frame 600: Queue=3, Total=10\n",
      "Frame 650: Queue=3, Total=9\n",
      "Frame 700: Queue=4, Total=8\n",
      "Frame 750: Queue=4, Total=8\n",
      "Frame 800: Queue=3, Total=6\n",
      "Frame 850: Queue=4, Total=9\n",
      "Frame 900: Queue=4, Total=8\n",
      "Frame 950: Queue=4, Total=8\n",
      "Frame 1000: Queue=4, Total=9\n",
      "Frame 1050: Queue=1, Total=1\n",
      "Frame 1100: Queue=4, Total=10\n",
      "Frame 1150: Queue=4, Total=10\n",
      "Frame 1200: Queue=4, Total=11\n",
      "Frame 1250: Queue=4, Total=9\n",
      "Frame 1300: Queue=3, Total=8\n",
      "Frame 1350: Queue=3, Total=8\n",
      "Frame 1400: Queue=3, Total=7\n",
      "Frame 1450: Queue=3, Total=8\n",
      "Frame 1500: Queue=3, Total=8\n",
      "Frame 1550: Queue=3, Total=8\n",
      "Frame 1600: Queue=2, Total=10\n",
      "Frame 1650: Queue=3, Total=8\n",
      "Frame 1700: Queue=3, Total=7\n",
      "Frame 1750: Queue=3, Total=7\n",
      "Frame 1800: Queue=3, Total=7\n",
      "Frame 1850: Queue=3, Total=7\n",
      "Frame 1900: Queue=3, Total=7\n",
      "Frame 1950: Queue=3, Total=6\n",
      "Frame 2000: Queue=3, Total=6\n",
      "Frame 2050: Queue=3, Total=6\n",
      "Frame 2100: Queue=3, Total=7\n",
      "Frame 2150: Queue=3, Total=7\n",
      "Frame 2200: Queue=3, Total=7\n",
      "Frame 2250: Queue=3, Total=7\n",
      "Frame 2300: Queue=3, Total=7\n",
      "Frame 2350: Queue=3, Total=7\n",
      "Frame 2400: Queue=3, Total=7\n",
      "Frame 2450: Queue=3, Total=8\n",
      "Frame 2500: Queue=3, Total=8\n",
      "Frame 2550: Queue=3, Total=8\n",
      "Frame 2600: Queue=3, Total=8\n",
      "Frame 2650: Queue=3, Total=8\n",
      "Frame 2700: Queue=3, Total=8\n",
      "Frame 2750: Queue=3, Total=8\n",
      "Frame 2800: Queue=3, Total=8\n",
      "Frame 2850: Queue=3, Total=8\n",
      "Frame 2900: Queue=3, Total=8\n",
      "Frame 2950: Queue=2, Total=7\n",
      "Frame 3000: Queue=2, Total=8\n",
      "Frame 3050: Queue=0, Total=0\n",
      "Frame 3100: Queue=0, Total=0\n",
      "Frame 3150: Queue=0, Total=0\n",
      "INFO: ../nvdsinfer/nvdsinfer_model_builder.cpp:363 [Implicit Engine Info]: layers num: 0\n",
      "\n",
      "nvstreammux: Successfully handled EOS for source_id=0\n",
      "End-of-stream\n",
      "\n",
      "Cleaning up...\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETED\n",
      " Time elapsed: 13.89 seconds\n",
      "Output saved to: /app/notebooks/queue_sizing_out.mp4\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING QUEUE SIZING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input: {INPUT_VIDEO_FILE}\")\n",
    "print(f\"Output: {OUTPUT_VIDEO_NAME}\")\n",
    "print(\"ROI: 60% of frame (center)\")\n",
    "print(\"Counting: People inside green ROI box\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Start pipeline\n",
    "ret = pipeline.set_state(Gst.State.PLAYING)\n",
    "if ret == Gst.StateChangeReturn.FAILURE:\n",
    "    print(\" ERROR: Unable to set pipeline to PLAYING state\")\n",
    "else:\n",
    "    try:\n",
    "        # Run event loop (blocks until EOS or error)\n",
    "        loop.run()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Error: {e}\")\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        print(\"\\nCleaning up...\")\n",
    "        pipeline.set_state(Gst.State.NULL)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"PIPELINE COMPLETED\")\n",
    "        print(f\" Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Output saved to: {OUTPUT_VIDEO_NAME}\")\n",
    "        print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Output file exists\n",
      "Location: /app/notebooks/queue_sizing_out.mp4\n",
      "Size: 47.17 MB\n",
      "\n",
      "On your host machine: ~/deepstream8/notebooks/ds_out.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists(OUTPUT_VIDEO_NAME):\n",
    "    file_size = os.path.getsize(OUTPUT_VIDEO_NAME)\n",
    "    print(f\" Output file exists\")\n",
    "    print(f\"Location: {OUTPUT_VIDEO_NAME}\")\n",
    "    print(f\"Size: {file_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"\\nOn your host machine: ~/deepstream8/notebooks/ds_out.mp4\")\n",
    "else:\n",
    "    print(f\"Output file not found: {OUTPUT_VIDEO_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align: center; margin: 20px;\">\n",
       "        <h3>ðŸŽ¬ Queue Sizing Detection Output</h3>\n",
       "        <video width=\"800\" controls>\n",
       "            <source src=\"queue_sizing_out.mp4\" type=\"video/mp4\">\n",
       "            Your browser does not support the video tag.\n",
       "        </video>\n",
       "        <p style=\"margin-top: 10px;\">\n",
       "            <strong>File:</strong> queue_sizing_out.mp4 | \n",
       "            <strong>Size:</strong> 47.17 MB\n",
       "        </p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display output video with HTML5 player\n",
    "\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "if os.path.exists(OUTPUT_VIDEO_NAME):\n",
    "    # Create HTML5 video player\n",
    "    html = f\"\"\"\n",
    "    <div style=\"text-align: center; margin: 20px;\">\n",
    "        <h3>ðŸŽ¬ Queue Sizing Detection Output</h3>\n",
    "        <video width=\"800\" controls>\n",
    "            <source src=\"queue_sizing_out.mp4\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "        <p style=\"margin-top: 10px;\">\n",
    "            <strong>File:</strong> queue_sizing_out.mp4 | \n",
    "            <strong>Size:</strong> {os.path.getsize(OUTPUT_VIDEO_NAME) / (1024*1024):.2f} MB\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "else:\n",
    "    print(f\"Video not found: {OUTPUT_VIDEO_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
